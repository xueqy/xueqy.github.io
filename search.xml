<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>bigdata</title>
    <url>/2020/06/27/bigdata/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/06/26/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
        <tag>表格</tag>
        <tag>表单验证</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/06/26/hello/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-06-27聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）</title>
    <url>/2020/06/27/hiveqingxie/</url>
    <content><![CDATA[<h3 id="聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）"><a href="#聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）" class="headerlink" title="聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）"></a>聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）</h3><h4 id="1-倾斜原因："><a href="#1-倾斜原因：" class="headerlink" title="1. 倾斜原因："></a>1. 倾斜原因：</h4><p>map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce 上的数据量差异过大。</p>
<p> （1）key分布不均匀;</p>
<p>（2）业务数据本身的特性;</p>
<p>（3）建表时考虑不周;</p>
<p>（4）某些SQL语句本身就有数据倾斜;</p>
<p>如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。</p>
<h4 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h4><h5 id="2-1-hive参数调节："><a href="#2-1-hive参数调节：" class="headerlink" title="2.1.hive参数调节："></a>2.1.hive参数调节：</h5><p>hive.map.aggr = true</p>
<p>hive.groupby.skewindata=true</p>
<p>有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<h5 id="2-2-SQL-语句调节："><a href="#2-2-SQL-语句调节：" class="headerlink" title="2.2 SQL 语句调节："></a>2.2 SQL 语句调节：</h5><p>① 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果。</p>
<p>② 大小表Join：<br>使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce.</p>
<p>③ 大表Join大表：<br>把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果。</p>
<p>④ count distinct大量相同特殊值:<br>count distinct 时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>mr与spark的区别是什么？</title>
    <url>/2020/06/27/mrSpark/</url>
    <content><![CDATA[<ul>
<li>MR是基于进程，spark是基于线程</li>
<li>Spark的多个task跑在同一个进程上，这个进程会伴随spark应用程序的整个生命周期，即使没有作业进行，进</li>
</ul>
<p>程也是存在的</p>
<ul>
<li>MR的每一个task都是一个进程，当task完成时，进程也会结束</li>
<li>所以，spark比MR快的原因也在这，MR启动就需要申请资源，用完就销毁，但是spark把进程拿到以后，这个进</li>
</ul>
<p>程会一直存在，即使没有job在跑，所以后边的job可以直接启动，不需要再重新申请资源</p>
<h4 id="速度"><a href="#速度" class="headerlink" title="速度"></a>速度</h4><p>spark把运算的中间数据存放在内存，迭代计算效率更高；MR的中间结果需要落地，需要保存到磁盘，这样必然</p>
<p>会有磁盘IO操作，影响性能</p>
<h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><p>spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的只读性</p>
<p>质的数据集，这些集合石弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重</p>
<p>建；MR的话容错可能只能重新计算了，成本较高</p>
<h3 id="适用面"><a href="#适用面" class="headerlink" title="适用面"></a>适用面</h3><p>spark更加通用，spark提供了transformation和action这两大类的多个功能的api，另外还有流式处理</p>
<p>sparkstreaming模块，图计算GraphX等；MR只提供了map和reduce两种操作，流计算以及其他模块的支持比较缺</p>
<p>乏</p>
<h3 id="框架和生态"><a href="#框架和生态" class="headerlink" title="框架和生态"></a>框架和生态</h3><p> Spark框架和生态更为复杂，首先由RDD、血缘lineage、执行时的有向无环图DAG、stage划分等等，</p>
<p>很多时候spark作业都需要根据不同的业务场景的需要进行调优，以达到性能要求，MR框架及其生态相对较为简</p>
<p>单，对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行</p>
<h3 id="运行环境："><a href="#运行环境：" class="headerlink" title="运行环境："></a>运行环境：</h3><ul>
<li><p>MR运行在YARN上，</p>
</li>
<li><p>spark</p>
</li>
</ul>
<blockquote>
<p>local：本地运行<br>standalone：使用Spark自带的资源管理框架，运行spark的应用<br>yarn：将spark应用类似mr一样，提交到yarn上运行<br>mesos：类似yarn的一种资源管理框架</p>
</blockquote>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>聊一聊MapReduce的Shuffle过程吧</title>
    <url>/2020/06/27/shuffle/</url>
    <content><![CDATA[<h3 id="聊一聊MapReduce的Shuffle过程吧"><a href="#聊一聊MapReduce的Shuffle过程吧" class="headerlink" title="聊一聊MapReduce的Shuffle过程吧"></a>聊一聊MapReduce的Shuffle过程吧</h3><p><img src="/medias/shuffle.png" alt="shuffle"><br>Map 方法之后 Reduce 方法之前这段处理过程叫 Shuffle<br>Map 方法之后，数据首先进入到分区方法，把数据标记好分区，然后把数据发送到<br>环形缓冲区；环形缓冲区默认大小 100m，环形缓冲区达到 80%时，进行溢写；溢写前对数<br>据进行排序，排序按照对 key 的索引进行字典顺序排序，排序的手段快排；溢写产生大量溢<br>写文件，需要对溢写文件进行归并排序；对溢写的文件也可以进行 Combiner 操作，前提是<br>汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待 Reduce 端拉取。<br>每个 Reduce 拉取 Map 端对应分区的数据。拉取数据后先存储到内存中，内存不够<br>了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。<br>在进入 Reduce 方法前，可以对数据进行分组操作。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么?</title>
    <url>/2020/06/27/sparka/</url>
    <content><![CDATA[<h3 id="Spark-master-HA-主从切换过程不会影响集群已有的作业运行，为什么"><a href="#Spark-master-HA-主从切换过程不会影响集群已有的作业运行，为什么" class="headerlink" title="Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么?"></a>Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么?</h3><p>&gt;  因为程序在运行之前，已经申请过资源了，driver和Executors通讯，不需要和master进行通讯的。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark中的RDD是什么，有哪些特性?</title>
    <url>/2020/06/27/sparkb/</url>
    <content><![CDATA[<h3 id="spark中的RDD是什么，有哪些特性"><a href="#spark中的RDD是什么，有哪些特性" class="headerlink" title="spark中的RDD是什么，有哪些特性?"></a>spark中的RDD是什么，有哪些特性?</h3><h5 id="1、RDD（Resilient-Distributed-Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。"><a href="#1、RDD（Resilient-Distributed-Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。" class="headerlink" title="1、RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。"></a>1、RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</h5><pre><code>- Dataset：就是一个集合，用于存放数据的
- Distributed：分布式，可以并行在集群计算
- Resilient：表示弹性的 
    弹性表示 
        1、RDD中的数据可以存储在内存或者是磁盘
        2、RDD中的分区是可以改变的</code></pre><h5 id="2、五大特性："><a href="#2、五大特性：" class="headerlink" title="2、五大特性："></a>2、五大特性：</h5><pre><code>（1）A list of partitions 
一个分区列表，RDD中的数据都存在一个分区列表里面
（2）A function for computing each split 
作用在每一个分区中的函数
（3）A list of dependencies on other RDDs 
一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的
（4）Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) 
可选的，针对于kv类型的RDD才具有这个特性，作用是决定了数据的来源以及数据处理后的去向
（5）Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) 
可选项，数据本地性，数据位置最优</code></pre>]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD中reduceBykey与groupByKey哪个性能好，为什么?</title>
    <url>/2020/06/27/sparkc/</url>
    <content><![CDATA[<h3 id="RDD中reduceBykey与groupByKey哪个性能好，为什么"><a href="#RDD中reduceBykey与groupByKey哪个性能好，为什么" class="headerlink" title="RDD中reduceBykey与groupByKey哪个性能好，为什么?"></a>RDD中reduceBykey与groupByKey哪个性能好，为什么?</h3><p>（1）groupByKey()是对RDD中的所有数据做shuffle,根据不同的Key映射到不同的partition中再进行aggregate。</p>
<p>（2）aggregateByKey()是先对每个partition中的数据根据不同的Key进行aggregate，然后将结果进行shuffle，完成各个partition之间的aggregate。因此，和groupByKey()相比，运算量小了很多。</p>
<p> (3)  distinct()也是对RDD中的所有数据做shuffle进行aggregate后再去重。</p>
<p>（4）reduceByKey()也是先在单台机器中计算，再将结果进行shuffle，减小运算量</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark streming在实时处理时会发生什么故障，如何停止，解决?</title>
    <url>/2020/06/27/sparkd/</url>
    <content><![CDATA[<h3 id="spark-streming在实时处理时会发生什么故障，如何停止，解决"><a href="#spark-streming在实时处理时会发生什么故障，如何停止，解决" class="headerlink" title="spark streming在实时处理时会发生什么故障，如何停止，解决?"></a>spark streming在实时处理时会发生什么故障，如何停止，解决?</h3><p>和Kafka整合时消息无序：</p>
<p>修改Kafka的ack参数，当ack=1时，master确认收到消息就算投递成功。ack=0时，不需要收到消息便算成功，高效不准确。ack=all，master和server都要受到消息才算成功，准确不高效。</p>
<p>StreamingContext.stop会把关联的SparkContext对象也停止，如果不想把SparkContext对象也停止的话可以把StremingContext.stop的可选参数stopSparkContext设为flase。一个SparkContext对象可以和多个streamingcontext对象关联。只要对前一个stremingcontext.stop(stopsparkcontext=false),然后再创建新的stremingcontext对象就可以了。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark streaming 读取kafka数据的两种方式?</title>
    <url>/2020/06/27/sparke/</url>
    <content><![CDATA[<h3 id="spark-streaming-读取kafka数据的两种方式"><a href="#spark-streaming-读取kafka数据的两种方式" class="headerlink" title="spark streaming 读取kafka数据的两种方式?"></a>spark streaming 读取kafka数据的两种方式?</h3><ul>
<li><strong>Receiver-base：</strong><br>使用Kafka的高层次Consumer API来实现。receiver从Kafka中获取的数据都存储在Spark Executor的内存中，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。</li>
<li><strong>Direct：</strong><br>Spark1.3中引入Direct方式，用来替代掉使用Receiver接收数据，这种方式会周期性地查询Kafka，获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark 如何防止内存溢出?</title>
    <url>/2020/06/27/sparkf/</url>
    <content><![CDATA[<h3 id="spark-如何防止内存溢出"><a href="#spark-如何防止内存溢出" class="headerlink" title="spark 如何防止内存溢出?"></a>spark 如何防止内存溢出?</h3><h4 id="driver端的内存溢出"><a href="#driver端的内存溢出" class="headerlink" title="driver端的内存溢出"></a>driver端的内存溢出</h4><p>可以增大driver的内存参数：spark.driver.memory (default 1g)<br>这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。</p>
<h4 id="map过程产生大量对象导致内存溢出"><a href="#map过程产生大量对象导致内存溢出" class="headerlink" title="map过程产生大量对象导致内存溢出"></a>map过程产生大量对象导致内存溢出</h4><p>这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。<br>面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区，不会有shuffle的过程。</p>
<h4 id="数据不平衡导致内存溢出"><a href="#数据不平衡导致内存溢出" class="headerlink" title="数据不平衡导致内存溢出"></a>数据不平衡导致内存溢出</h4><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。</p>
<h4 id="shuffle后内存溢出"><a href="#shuffle后内存溢出" class="headerlink" title="shuffle后内存溢出"></a>shuffle后内存溢出</h4><p>shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。</p>
<h4 id="standalone模式下资源分配不均匀导致内存溢出"><a href="#standalone模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone模式下资源分配不均匀导致内存溢出"></a>standalone模式下资源分配不均匀导致内存溢出</h4><p>在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p>
<h4 id="使用rdd-persist-StorageLevel-MEMORY-AND-DISK-SER-代替rdd-cache"><a href="#使用rdd-persist-StorageLevel-MEMORY-AND-DISK-SER-代替rdd-cache" class="headerlink" title="使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()"></a>使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()</h4><p>rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark有哪些优化方法?</title>
    <url>/2020/06/27/sparkg/</url>
    <content><![CDATA[<h3 id="Spark有哪些优化方法"><a href="#Spark有哪些优化方法" class="headerlink" title="Spark有哪些优化方法?"></a>Spark有哪些优化方法?</h3><h4 id="spark调优比较复杂，但是大体可以分为三个方面来进行"><a href="#spark调优比较复杂，但是大体可以分为三个方面来进行" class="headerlink" title="spark调优比较复杂，但是大体可以分为三个方面来进行"></a>spark调优比较复杂，但是大体可以分为三个方面来进行</h4><p>1）平台层面的调优：防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet</p>
<p>2）应用程序层面的调优：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等</p>
<p>3）JVM层面的调优：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction&#x3D;0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction&#x3D;0.3 \</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>如何配置spark master的HA？</title>
    <url>/2020/06/27/sparkha/</url>
    <content><![CDATA[<h3 id="如何配置spark-master的HA？"><a href="#如何配置spark-master的HA？" class="headerlink" title="如何配置spark master的HA？"></a>如何配置spark master的HA？</h3><p>1)配置zookeeper</p>
<p>2)修改spark_env.sh文件,spark的master参数不在指定，添加如下代码到各个master节点<br>  export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk01:2181,zk02:2181,zk03:2181 -Dspark.deploy.zookeeper.dir=/spark”</p>
<p>3) 将spark_env.sh分发到各个节点</p>
<p>4)找到一个master节点，执行./start-all.sh，会在这里启动主master,其他的master备节点，启动master命令: ./sbin/start-master.sh</p>
<p>5)提交程序的时候指定master的时候要指定三台master，例如<br>./spark-shell –master spark://master01:7077,master02:7077,master03:7077</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-06-28聊一聊你所遇到的mysql面试问题（☆☆☆☆☆）</title>
    <url>/2020/06/28/mysqlindex/</url>
    <content><![CDATA[<h4 id="索引相关"><a href="#索引相关" class="headerlink" title="索引相关"></a>索引相关</h4><p>Q：那你能说说什么是索引吗？</p>
<p>A：索引其实是一种数据结构，能够帮助我们快速的检索数据库中的数据</p>
<p>Q：那么索引具体采用的哪种数据结构呢？ </p>
<p>A：常见的MySQL主要有两种结构：Hash索引和B+ Tree索引，我们使用的是InnoDB引擎，默认的是B+树</p>
<blockquote>
<p>这里我耍了一个小心机，特意说了一下索引和存储引擎有关。希望面试官可以问我一些关于存储引擎的问题。然而面试官并没有被我带跑…</p>
</blockquote>
<p>Q：既然你提到InnoDB使用的B+ 树的索引模型，那么你知道为什么采用B+ 树吗？这和Hash索引比较起来有什么优缺点吗？</p>
<p>A：（突然觉得这道题有点难，但是我还是凭借着自己的知识储备简单的回答上一些）因为Hash索引底层是哈希表，哈希表是一种以key-value存储数据的结构，所以多个数据在存储关系上是完全没有任何顺序关系的，所以，对于区间查询是无法直接通过索引查询的，就需要全表扫描。所以，哈希索引只适用于等值查询的场景。而B+ 树是一种多路平衡查询树，所以他的节点是天然有序的（左子节点小于父节点、父节点小于右子节点），所以对于范围查询的时候不需要做全表扫描</p>
<p>Q：除了上面这个范围查询的，你还能说出其他的一些区别吗？ </p>
<p>A：</p>
<blockquote>
<p>B+ Tree索引和Hash索引区别？<br>哈希索引适合等值查询，但是无法进行范围查询<br>哈希索引没办法利用索引完成排序<br>哈希索引不支持多列联合索引的最左匹配规则<br>如果有大量重复键值的情况下，哈希索引的效率会很低，因为存在哈希碰撞问题</p>
</blockquote>
<h4 id="聚簇索引、覆盖索引"><a href="#聚簇索引、覆盖索引" class="headerlink" title="聚簇索引、覆盖索引"></a><strong>聚簇索引、覆盖索引</strong></h4><p>Q：刚刚我们聊到B+ Tree ，那你知道B+ Tree的叶子节点都可以存哪些东西吗？</p>
<p>A：InnoDB的B+ Tree可能存储的是整行数据，也有可能是主键的值</p>
<p>Q：那这两者有什么区别吗？ </p>
<p>A：（当他问我叶子节点的时候，其实我就猜到他可能要问我聚簇索引和非聚簇索引了）在 InnoDB 里，索引B+ Tree的叶子节点存储了整行数据的是主键索引，也被称之为聚簇索引。而索引B+ Tree的叶子节点存储了主键的值的是非主键索引，也被称之为非聚簇索引</p>
<p>Q：那么，聚簇索引和非聚簇索引，在查询数据的时候有区别吗？</p>
<p>A：聚簇索引查询会更快？</p>
<p>Q：为什么呢？ </p>
<p>A：因为主键索引树的叶子节点直接就是我们要查询的整行数据了。而非主键索引的叶子节点是主键的值，查到主键的值以后，还需要再通过主键的值再进行一次查询</p>
<p>Q：刚刚你提到主键索引查询只会查一次，而非主键索引需要回表查询多次。（后来我才知道，原来这个过程叫做回表）是所有情况都是这样的吗？非主键索引一定会查询多次吗？</p>
<p>A：（额、这个问题我回答的不好，后来我自己查资料才知道，通过覆盖索引也可以只查询一次）</p>
<h4 id="覆盖索引？"><a href="#覆盖索引？" class="headerlink" title="覆盖索引？"></a><strong>覆盖索引？</strong></h4><p>覆盖索引（covering index）指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。</p>
<p>当一条查询语句符合覆盖索引条件时，MySQL只需要通过索引就可以返回查询所需要的数据，这样避免了查到索引后再返回表操作，减少I/O提高效率。</p>
<p>如，表covering_index_sample中有一个普通索引 idx_key1_key2(key1,key2)。</p>
<p>当我们通过SQL语句：select key2 from covering_index_sample where key1 = ‘keytest’;的时候，就可以通过覆盖索引查询，无需回表。</p>
<h4 id="联合索引、最左前缀匹配"><a href="#联合索引、最左前缀匹配" class="headerlink" title="联合索引、最左前缀匹配"></a><strong>联合索引、最左前缀匹配</strong></h4><p>Q：不知道的话没关系，想问一下，你们在创建索引的时候都会考虑哪些因素呢？</p>
<p>A：我们一般对于查询概率比较高，经常作为where条件的字段设置索引</p>
<p>Q： 那你们有用过联合索引吗？ </p>
<p>A：用过呀，我们有对一些表中创建过联合索引</p>
<p>Q：那你们在创建联合索引的时候，需要做联合索引多个字段之间顺序你们是如何选择的呢？ </p>
<p>A：我们把识别度最高的字段放到最前面</p>
<p>Q：为什么这么做呢？</p>
<p>A：（这个问题有点把我问蒙了，稍微有些慌乱）这样的话可能命中率会高一点吧。。。</p>
<p>Q： 那你知道最左前缀匹配吗？</p>
<p>A：（我突然想起来原来面试官是想问这个，怪自己刚刚为什么就没想到这个呢。）哦哦哦。您刚刚问的是这个意思啊，在创建多列索引时，我们根据业务需求，where子句中使用最频繁的一列放在最左边，因为MySQL索引查询会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。所以当我们创建一个联合索引的时候，如(key1,key2,key3)，相当于创建了（key1）、(key1,key2)和(key1,key2,key3)三个索引，这就是最左匹配原则</p>
<p>虽然我一开始有点懵，没有联想到最左前缀匹配，但是面试官还是引导了我。很友善。</p>
<h4 id="索引下推、查询优化"><a href="#索引下推、查询优化" class="headerlink" title="索引下推、查询优化"></a><strong>索引下推、查询优化</strong></h4><p>Q：你们线上用的MySQL是哪个版本啊呢？ </p>
<p>A：我们MySQL是5.7 </p>
<p>Q：那你知道在MySQL 5.6中，对索引做了哪些优化吗？ </p>
<p>A：不好意思，这个我没有去了解过。（事后我查了一下，有一个比较重要的 ：Index Condition Pushdown Optimization）</p>
<p>Index Condition Pushdown（索引下推）</p>
<p>MySQL 5.6引入了索引下推优化，默认开启，使用SET optimizer_switch = ‘index_condition_pushdown=off’;可以将其关闭。官方文档中给的例子和解释如下：</p>
<p>people表中（zipcode，lastname，firstname）构成一个索引</p>
<p>SELECT * FROM people WHERE zipcode=’95054’ AND lastname LIKE ‘%etrunia%’ AND address LIKE ‘%Main Street%’;</p>
<p>如果没有使用索引下推技术，则MySQL会通过zipcode=’95054’从存储引擎中查询对应的数据，返回到MySQL服务端，然后MySQL服务端基于lastname LIKE ‘%etrunia%’和address LIKE ‘%Main Street%’来判断数据是否符合条件。</p>
<p>如果使用了索引下推技术，则MYSQL首先会返回符合zipcode=’95054’的索引，然后根据lastname LIKE ‘%etrunia%’和address LIKE ‘%Main Street%’来判断索引是否符合条件。如果符合条件，则根据该索引来定位对应的数据，如果不符合，则直接reject掉。有了索引下推优化，可以在有like条件查询的情况下，减少回表次数。</p>
<p>Q：你们创建的那么多索引，到底有没有生效，或者说你们的SQL语句有没有使用索引查询你们有统计过吗？</p>
<p>A：这个还没有统计过，除非遇到慢SQL的时候我们才会去排查 </p>
<p>Q：那排查的时候，有什么手段可以知道有没有走索引查询呢？</p>
<p>A：可以通过explain查看sql语句的执行计划，通过执行计划来分析索引使用情况</p>
<p>Q：那什么情况下会发生明明创建了索引，但是执行的时候并没有通过索引呢？ </p>
<p>A：（大概记得和优化器有关，但是这个问题并没有回答好）</p>
<h4 id="查询优化器？"><a href="#查询优化器？" class="headerlink" title="查询优化器？"></a>查询优化器？</h4><p>一条SQL语句的查询，可以有不同的执行方案，至于最终选择哪种方案，需要通过优化器进行选择，选择执行成本最低的方案。</p>
<p>在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案。</p>
<p>这个成本最低的方案就是所谓的执行计划。优化过程大致如下：</p>
<p>1、根据搜索条件，找出所有可能使用的索引 </p>
<p>2、计算全表扫描的代价 </p>
<p>3、计算使用不同索引执行查询的代价 </p>
<p>4、对比各种执行方案的代价，找出成本最低的那一个</p>
<p>Q：哦，索引有关的知识我们暂时就问这么多吧。你们线上数据的事务隔离级别是什么呀？ </p>
<p>A：(后面关于事务隔离级别的问题了，就不展开了)<br>Read Uncommitted（读取未提交内容）</p>
<p>在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。</p>
<p>Read Committed（读取提交内容）</p>
<p>这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。</p>
<p>Repeatable Read（可重读）</p>
<p>这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。</p>
<p>Serializable（可串行化）</p>
<p>这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。</p>
<p>出现问题</p>
<p>这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：</p>
<p>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</p>
<p>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</p>
<p>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</p>
<h4 id="查询相关"><a href="#查询相关" class="headerlink" title="查询相关"></a>查询相关</h4>]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-06-28说一说你了解的Mapreduce过程吧</title>
    <url>/2020/06/28/mapreduce/</url>
    <content><![CDATA[<h4 id="Map-task"><a href="#Map-task" class="headerlink" title="Map task"></a>Map task</h4><ul>
<li><p>由程序内的InputFormat(默认实现类TextInputFormat)来读取外部数据，它会调用RecordReader(它的成员变量)的read()方法来读取，返回k,v键值对。</p>
</li>
<li><p>读取的k,v键值对传送给map()方法，作为其入参来执行用户定义的map逻辑。</p>
</li>
<li><p>context.write方法被调用时，outputCollector组件会将map()方法的输出结果写入到环形缓冲区内。</p>
</li>
<li><p>环形缓冲区其实就是一个数组，后端不断接受数据的同时，前端数据不断被溢出，长度用完后读取的新数据再从前端开始覆盖。这个缓冲区默认大小100M，可以通过MR.SORT.MB(应该是它)配置。</p>
</li>
<li><p>spiller组件会从环形缓冲区溢出文件，这过程会按照定义的partitioner分区(默认是hashpartition)，并且按照key.compareTo进行排序(底层主要用快排和外部排序)，若有combiner也会执行combiner。spiller的不断工作，会不断溢出许多小文件。这些文件仍在map task所处机器上。</p>
</li>
<li><p>小文件执行merge(合并)，行程分区且区内有序的大文件(归并排序，会再一次调用combiner)。</p>
</li>
<li><p>Reduce会根据自己的分区，去所有map task中，从文件读取对应的数据。</p>
</li>
</ul>
<h4 id="Reduce-task"><a href="#Reduce-task" class="headerlink" title="Reduce task"></a>Reduce task</h4><ul>
<li><p>.reduce task通过网络向map task获取某一分区的数据。</p>
</li>
<li><p>通过GroupingComparator()分辨同一组的数据，把他们发送给reduce(k,iterator)方法</p>
</li>
</ul>
<blockquote>
<p> (这里多个数据合成一组时，只取其中一个key，取得是第一个)。</p>
</blockquote>
<ul>
<li><p>调用context.write()方法，会让OutPurFormat调用RecodeWriter的write()方法将处理结果写入到数据仓库中。写出的只有一个分区的文件数据.</p>
</li>
<li><p>就此mapreduce的工作结束，其中map的context.write()调用后，开始聚合数据写入到reduce的过程叫做Shuffle，是mapreduce的核心所在。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
</search>
