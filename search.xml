<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>bigdata</title>
    <url>/2020/06/27/bigdata/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/06/26/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>jQuery</tag>
        <tag>表格</tag>
        <tag>表单验证</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/06/26/hello/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-06-27聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）</title>
    <url>/2020/06/27/hiveqingxie/</url>
    <content><![CDATA[<h3 id="聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）"><a href="#聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）" class="headerlink" title="聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）"></a>聊一聊你所遇到的数据倾斜问题（☆☆☆☆☆）</h3><h4 id="1-倾斜原因："><a href="#1-倾斜原因：" class="headerlink" title="1. 倾斜原因："></a>1. 倾斜原因：</h4><p>map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce 上的数据量差异过大。</p>
<p> （1）key分布不均匀;</p>
<p>（2）业务数据本身的特性;</p>
<p>（3）建表时考虑不周;</p>
<p>（4）某些SQL语句本身就有数据倾斜;</p>
<p>如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。</p>
<h4 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h4><h5 id="2-1-hive参数调节："><a href="#2-1-hive参数调节：" class="headerlink" title="2.1.hive参数调节："></a>2.1.hive参数调节：</h5><p>hive.map.aggr = true</p>
<p>hive.groupby.skewindata=true</p>
<p>有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中），最后完成最终的聚合操作。</p>
<h5 id="2-2-SQL-语句调节："><a href="#2-2-SQL-语句调节：" class="headerlink" title="2.2 SQL 语句调节："></a>2.2 SQL 语句调节：</h5><p>① 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果。</p>
<p>② 大小表Join：<br>使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce.</p>
<p>③ 大表Join大表：<br>把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果。</p>
<p>④ count distinct大量相同特殊值:<br>count distinct 时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>mr与spark的区别是什么？</title>
    <url>/2020/06/27/mrSpark/</url>
    <content><![CDATA[<ul>
<li>MR是基于进程，spark是基于线程</li>
<li>Spark的多个task跑在同一个进程上，这个进程会伴随spark应用程序的整个生命周期，即使没有作业进行，进</li>
</ul>
<p>程也是存在的</p>
<ul>
<li>MR的每一个task都是一个进程，当task完成时，进程也会结束</li>
<li>所以，spark比MR快的原因也在这，MR启动就需要申请资源，用完就销毁，但是spark把进程拿到以后，这个进</li>
</ul>
<p>程会一直存在，即使没有job在跑，所以后边的job可以直接启动，不需要再重新申请资源</p>
<h4 id="速度"><a href="#速度" class="headerlink" title="速度"></a>速度</h4><p>spark把运算的中间数据存放在内存，迭代计算效率更高；MR的中间结果需要落地，需要保存到磁盘，这样必然</p>
<p>会有磁盘IO操作，影响性能</p>
<h3 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h3><p>spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在节点内存中的只读性</p>
<p>质的数据集，这些集合石弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重</p>
<p>建；MR的话容错可能只能重新计算了，成本较高</p>
<h3 id="适用面"><a href="#适用面" class="headerlink" title="适用面"></a>适用面</h3><p>spark更加通用，spark提供了transformation和action这两大类的多个功能的api，另外还有流式处理</p>
<p>sparkstreaming模块，图计算GraphX等；MR只提供了map和reduce两种操作，流计算以及其他模块的支持比较缺</p>
<p>乏</p>
<h3 id="框架和生态"><a href="#框架和生态" class="headerlink" title="框架和生态"></a>框架和生态</h3><p> Spark框架和生态更为复杂，首先由RDD、血缘lineage、执行时的有向无环图DAG、stage划分等等，</p>
<p>很多时候spark作业都需要根据不同的业务场景的需要进行调优，以达到性能要求，MR框架及其生态相对较为简</p>
<p>单，对性能的要求也相对较弱，但是运行较为稳定，适合长期后台运行</p>
<h3 id="运行环境："><a href="#运行环境：" class="headerlink" title="运行环境："></a>运行环境：</h3><ul>
<li><p>MR运行在YARN上，</p>
</li>
<li><p>spark</p>
</li>
</ul>
<blockquote>
<p>local：本地运行<br>standalone：使用Spark自带的资源管理框架，运行spark的应用<br>yarn：将spark应用类似mr一样，提交到yarn上运行<br>mesos：类似yarn的一种资源管理框架</p>
</blockquote>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>聊一聊MapReduce的Shuffle过程吧</title>
    <url>/2020/06/27/shuffle/</url>
    <content><![CDATA[<h3 id="聊一聊MapReduce的Shuffle过程吧"><a href="#聊一聊MapReduce的Shuffle过程吧" class="headerlink" title="聊一聊MapReduce的Shuffle过程吧"></a>聊一聊MapReduce的Shuffle过程吧</h3><p><img src="/medias/bigdata/shuffle.png" alt="shuffle"><br>Map 方法之后 Reduce 方法之前这段处理过程叫 Shuffle<br>Map 方法之后，数据首先进入到分区方法，把数据标记好分区，然后把数据发送到<br>环形缓冲区；环形缓冲区默认大小 100m，环形缓冲区达到 80%时，进行溢写；溢写前对数<br>据进行排序，排序按照对 key 的索引进行字典顺序排序，排序的手段快排；溢写产生大量溢<br>写文件，需要对溢写文件进行归并排序；对溢写的文件也可以进行 Combiner 操作，前提是<br>汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待 Reduce 端拉取。<br>每个 Reduce 拉取 Map 端对应分区的数据。拉取数据后先存储到内存中，内存不够<br>了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。<br>在进入 Reduce 方法前，可以对数据进行分组操作。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么?</title>
    <url>/2020/06/27/sparka/</url>
    <content><![CDATA[<h3 id="Spark-master-HA-主从切换过程不会影响集群已有的作业运行，为什么"><a href="#Spark-master-HA-主从切换过程不会影响集群已有的作业运行，为什么" class="headerlink" title="Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么?"></a>Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么?</h3><p>&gt;  因为程序在运行之前，已经申请过资源了，driver和Executors通讯，不需要和master进行通讯的。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark中的RDD是什么，有哪些特性?</title>
    <url>/2020/06/27/sparkb/</url>
    <content><![CDATA[<h3 id="spark中的RDD是什么，有哪些特性"><a href="#spark中的RDD是什么，有哪些特性" class="headerlink" title="spark中的RDD是什么，有哪些特性?"></a>spark中的RDD是什么，有哪些特性?</h3><h5 id="1、RDD（Resilient-Distributed-Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。"><a href="#1、RDD（Resilient-Distributed-Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。" class="headerlink" title="1、RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。"></a>1、RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。</h5><ul>
<li>Dataset：就是一个集合，用于存放数据的</li>
<li>Distributed：分布式，可以并行在集群计算</li>
<li>Resilient：表示弹性的<br>  弹性表示<br>1、RDD中的数据可以存储在内存或者是磁盘<br>2、RDD中的分区是可以改变的<h5 id="2、五大特性："><a href="#2、五大特性：" class="headerlink" title="2、五大特性："></a>2、五大特性：</h5>（1）A list of partitions<br>一个分区列表，RDD中的数据都存在一个分区列表里面<br>（2）A function for computing each split<br>作用在每一个分区中的函数<br>（3）A list of dependencies on other RDDs<br>一个RDD依赖于其他多个RDD，这个点很重要，RDD的容错机制就是依据这个特性而来的<br>（4）Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>可选的，针对于kv类型的RDD才具有这个特性，作用是决定了数据的来源以及数据处理后的去向<br>（5）Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)<br>可选项，数据本地性，数据位置最优</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD中reduceBykey与groupByKey哪个性能好，为什么?</title>
    <url>/2020/06/27/sparkc/</url>
    <content><![CDATA[<h3 id="RDD中reduceBykey与groupByKey哪个性能好，为什么"><a href="#RDD中reduceBykey与groupByKey哪个性能好，为什么" class="headerlink" title="RDD中reduceBykey与groupByKey哪个性能好，为什么?"></a>RDD中reduceBykey与groupByKey哪个性能好，为什么?</h3><p>（1）groupByKey()是对RDD中的所有数据做shuffle,根据不同的Key映射到不同的partition中再进行aggregate。</p>
<p>（2）aggregateByKey()是先对每个partition中的数据根据不同的Key进行aggregate，然后将结果进行shuffle，完成各个partition之间的aggregate。因此，和groupByKey()相比，运算量小了很多。</p>
<p> (3)  distinct()也是对RDD中的所有数据做shuffle进行aggregate后再去重。</p>
<p>（4）reduceByKey()也是先在单台机器中计算，再将结果进行shuffle，减小运算量</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark streming在实时处理时会发生什么故障，如何停止，解决?</title>
    <url>/2020/06/27/sparkd/</url>
    <content><![CDATA[<h3 id="spark-streming在实时处理时会发生什么故障，如何停止，解决"><a href="#spark-streming在实时处理时会发生什么故障，如何停止，解决" class="headerlink" title="spark streming在实时处理时会发生什么故障，如何停止，解决?"></a>spark streming在实时处理时会发生什么故障，如何停止，解决?</h3><p>和Kafka整合时消息无序：</p>
<p>修改Kafka的ack参数，当ack=1时，master确认收到消息就算投递成功。ack=0时，不需要收到消息便算成功，高效不准确。ack=all，master和server都要受到消息才算成功，准确不高效。</p>
<p>StreamingContext.stop会把关联的SparkContext对象也停止，如果不想把SparkContext对象也停止的话可以把StremingContext.stop的可选参数stopSparkContext设为flase。一个SparkContext对象可以和多个streamingcontext对象关联。只要对前一个stremingcontext.stop(stopsparkcontext=false),然后再创建新的stremingcontext对象就可以了。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark streaming 读取kafka数据的两种方式?</title>
    <url>/2020/06/27/sparke/</url>
    <content><![CDATA[<h3 id="spark-streaming-读取kafka数据的两种方式"><a href="#spark-streaming-读取kafka数据的两种方式" class="headerlink" title="spark streaming 读取kafka数据的两种方式?"></a>spark streaming 读取kafka数据的两种方式?</h3><ul>
<li><strong>Receiver-base：</strong><br>使用Kafka的高层次Consumer API来实现。receiver从Kafka中获取的数据都存储在Spark Executor的内存中，然后Spark Streaming启动的job会去处理那些数据。然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。</li>
<li><strong>Direct：</strong><br>Spark1.3中引入Direct方式，用来替代掉使用Receiver接收数据，这种方式会周期性地查询Kafka，获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>spark 如何防止内存溢出?</title>
    <url>/2020/06/27/sparkf/</url>
    <content><![CDATA[<h3 id="spark-如何防止内存溢出"><a href="#spark-如何防止内存溢出" class="headerlink" title="spark 如何防止内存溢出?"></a>spark 如何防止内存溢出?</h3><h4 id="driver端的内存溢出"><a href="#driver端的内存溢出" class="headerlink" title="driver端的内存溢出"></a>driver端的内存溢出</h4><p>可以增大driver的内存参数：spark.driver.memory (default 1g)<br>这个参数用来设置Driver的内存。在Spark程序中，SparkContext，DAGScheduler都是运行在Driver端的。对应rdd的Stage切分也是在Driver端运行，如果用户自己写的程序有过多的步骤，切分出过多的Stage，这部分信息消耗的是Driver的内存，这个时候就需要调大Driver的内存。</p>
<h4 id="map过程产生大量对象导致内存溢出"><a href="#map过程产生大量对象导致内存溢出" class="headerlink" title="map过程产生大量对象导致内存溢出"></a>map过程产生大量对象导致内存溢出</h4><p>这种溢出的原因是在单个map中产生了大量的对象导致的，例如：rdd.map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)，这个操作在rdd中，每个对象都产生了10000个对象，这肯定很容易产生内存溢出的问题。针对这种问题，在不增加内存的情况下，可以通过减少每个Task的大小，以便达到每个Task即使产生大量的对象Executor的内存也能够装得下。具体做法可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map。例如：rdd.repartition(10000).map(x=&gt;for(i &lt;- 1 to 10000) yield i.toString)。<br>面对这种问题注意，不能使用rdd.coalesce方法，这个方法只能减少分区，不能增加分区，不会有shuffle的过程。</p>
<h4 id="数据不平衡导致内存溢出"><a href="#数据不平衡导致内存溢出" class="headerlink" title="数据不平衡导致内存溢出"></a>数据不平衡导致内存溢出</h4><p>数据不平衡除了有可能导致内存溢出外，也有可能导致性能的问题，解决方法和上面说的类似，就是调用repartition重新分区。这里就不再累赘了。</p>
<h4 id="shuffle后内存溢出"><a href="#shuffle后内存溢出" class="headerlink" title="shuffle后内存溢出"></a>shuffle后内存溢出</h4><p>shuffle内存溢出的情况可以说都是shuffle后，单个文件过大导致的。在Spark中，join，reduceByKey这一类型的过程，都会有shuffle的过程，在shuffle的使用，需要传入一个partitioner，大部分Spark中的shuffle操作，默认的partitioner都是HashPatitioner，默认值是父RDD中最大的分区数,这个参数通过spark.default.parallelism控制(在spark-sql中用spark.sql.shuffle.partitions) ， spark.default.parallelism参数只对HashPartitioner有效，所以如果是别的Partitioner或者自己实现的Partitioner就不能使用spark.default.parallelism这个参数来控制shuffle的并发量了。如果是别的partitioner导致的shuffle内存溢出，就需要从partitioner的代码增加partitions的数量。</p>
<h4 id="standalone模式下资源分配不均匀导致内存溢出"><a href="#standalone模式下资源分配不均匀导致内存溢出" class="headerlink" title="standalone模式下资源分配不均匀导致内存溢出"></a>standalone模式下资源分配不均匀导致内存溢出</h4><p>在standalone的模式下如果配置了–total-executor-cores 和 –executor-memory 这两个参数，但是没有配置–executor-cores这个参数的话，就有可能导致，每个Executor的memory是一样的，但是cores的数量不同，那么在cores数量多的Executor中，由于能够同时执行多个Task，就容易导致内存溢出的情况。这种情况的解决方法就是同时配置–executor-cores或者spark.executor.cores参数，确保Executor资源分配均匀。</p>
<h4 id="使用rdd-persist-StorageLevel-MEMORY-AND-DISK-SER-代替rdd-cache"><a href="#使用rdd-persist-StorageLevel-MEMORY-AND-DISK-SER-代替rdd-cache" class="headerlink" title="使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()"></a>使用rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)代替rdd.cache()</h4><p>rdd.cache()和rdd.persist(Storage.MEMORY_ONLY)是等价的，在内存不足的时候rdd.cache()的数据会丢失，再次使用的时候会重算，而rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)在内存不足的时候会存储在磁盘，避免重算，只是消耗点IO时间。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark有哪些优化方法?</title>
    <url>/2020/06/27/sparkg/</url>
    <content><![CDATA[<h3 id="Spark有哪些优化方法"><a href="#Spark有哪些优化方法" class="headerlink" title="Spark有哪些优化方法?"></a>Spark有哪些优化方法?</h3><h4 id="spark调优比较复杂，但是大体可以分为三个方面来进行"><a href="#spark调优比较复杂，但是大体可以分为三个方面来进行" class="headerlink" title="spark调优比较复杂，但是大体可以分为三个方面来进行"></a>spark调优比较复杂，但是大体可以分为三个方面来进行</h4><p>1）平台层面的调优：防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet</p>
<p>2）应用程序层面的调优：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等</p>
<p>3）JVM层面的调优：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等</p>
<pre><code>./bin/spark-submit \
  --master yarn-cluster \
  --num-executors 100 \
  --executor-memory 6G \
  --executor-cores 4 \
  --driver-memory 1G \
  --conf spark.default.parallelism=1000 \
  --conf spark.storage.memoryFraction=0.5 \
  --conf spark.shuffle.memoryFraction=0.3 \</code></pre>]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>如何配置spark master的HA？</title>
    <url>/2020/06/27/sparkha/</url>
    <content><![CDATA[<h3 id="如何配置spark-master的HA？"><a href="#如何配置spark-master的HA？" class="headerlink" title="如何配置spark master的HA？"></a>如何配置spark master的HA？</h3><p>1)配置zookeeper</p>
<p>2)修改spark_env.sh文件,spark的master参数不在指定，添加如下代码到各个master节点<br>  export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk01:2181,zk02:2181,zk03:2181 -Dspark.deploy.zookeeper.dir=/spark”</p>
<p>3) 将spark_env.sh分发到各个节点</p>
<p>4)找到一个master节点，执行./start-all.sh，会在这里启动主master,其他的master备节点，启动master命令: ./sbin/start-master.sh</p>
<p>5)提交程序的时候指定master的时候要指定三台master，例如<br>./spark-shell –master spark://master01:7077,master02:7077,master03:7077</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-06-28聊一聊你所遇到的mysql面试问题（☆☆☆☆☆）</title>
    <url>/2020/06/28/mysqlindex/</url>
    <content><![CDATA[<h4 id="索引相关"><a href="#索引相关" class="headerlink" title="索引相关"></a>索引相关</h4><p>Q：那你能说说什么是索引吗？</p>
<p>A：索引其实是一种数据结构，能够帮助我们快速的检索数据库中的数据</p>
<p>Q：那么索引具体采用的哪种数据结构呢？ </p>
<p>A：常见的MySQL主要有两种结构：Hash索引和B+ Tree索引，我们使用的是InnoDB引擎，默认的是B+树</p>
<blockquote>
<p>这里我耍了一个小心机，特意说了一下索引和存储引擎有关。希望面试官可以问我一些关于存储引擎的问题。然而面试官并没有被我带跑…</p>
</blockquote>
<p>Q：既然你提到InnoDB使用的B+ 树的索引模型，那么你知道为什么采用B+ 树吗？这和Hash索引比较起来有什么优缺点吗？</p>
<p>A：（突然觉得这道题有点难，但是我还是凭借着自己的知识储备简单的回答上一些）因为Hash索引底层是哈希表，哈希表是一种以key-value存储数据的结构，所以多个数据在存储关系上是完全没有任何顺序关系的，所以，对于区间查询是无法直接通过索引查询的，就需要全表扫描。所以，哈希索引只适用于等值查询的场景。而B+ 树是一种多路平衡查询树，所以他的节点是天然有序的（左子节点小于父节点、父节点小于右子节点），所以对于范围查询的时候不需要做全表扫描</p>
<p>Q：除了上面这个范围查询的，你还能说出其他的一些区别吗？ </p>
<p>A：</p>
<blockquote>
<p>B+ Tree索引和Hash索引区别？<br>哈希索引适合等值查询，但是无法进行范围查询<br>哈希索引没办法利用索引完成排序<br>哈希索引不支持多列联合索引的最左匹配规则<br>如果有大量重复键值的情况下，哈希索引的效率会很低，因为存在哈希碰撞问题</p>
</blockquote>
<h4 id="聚簇索引、覆盖索引"><a href="#聚簇索引、覆盖索引" class="headerlink" title="聚簇索引、覆盖索引"></a><strong>聚簇索引、覆盖索引</strong></h4><p>Q：刚刚我们聊到B+ Tree ，那你知道B+ Tree的叶子节点都可以存哪些东西吗？</p>
<p>A：InnoDB的B+ Tree可能存储的是整行数据，也有可能是主键的值</p>
<p>Q：那这两者有什么区别吗？ </p>
<p>A：（当他问我叶子节点的时候，其实我就猜到他可能要问我聚簇索引和非聚簇索引了）在 InnoDB 里，索引B+ Tree的叶子节点存储了整行数据的是主键索引，也被称之为聚簇索引。而索引B+ Tree的叶子节点存储了主键的值的是非主键索引，也被称之为非聚簇索引</p>
<p>Q：那么，聚簇索引和非聚簇索引，在查询数据的时候有区别吗？</p>
<p>A：聚簇索引查询会更快？</p>
<p>Q：为什么呢？ </p>
<p>A：因为主键索引树的叶子节点直接就是我们要查询的整行数据了。而非主键索引的叶子节点是主键的值，查到主键的值以后，还需要再通过主键的值再进行一次查询</p>
<p>Q：刚刚你提到主键索引查询只会查一次，而非主键索引需要回表查询多次。（后来我才知道，原来这个过程叫做回表）是所有情况都是这样的吗？非主键索引一定会查询多次吗？</p>
<p>A：（额、这个问题我回答的不好，后来我自己查资料才知道，通过覆盖索引也可以只查询一次）</p>
<h4 id="覆盖索引？"><a href="#覆盖索引？" class="headerlink" title="覆盖索引？"></a><strong>覆盖索引？</strong></h4><p>覆盖索引（covering index）指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。</p>
<p>当一条查询语句符合覆盖索引条件时，MySQL只需要通过索引就可以返回查询所需要的数据，这样避免了查到索引后再返回表操作，减少I/O提高效率。</p>
<p>如，表covering_index_sample中有一个普通索引 idx_key1_key2(key1,key2)。</p>
<p>当我们通过SQL语句：select key2 from covering_index_sample where key1 = ‘keytest’;的时候，就可以通过覆盖索引查询，无需回表。</p>
<h4 id="联合索引、最左前缀匹配"><a href="#联合索引、最左前缀匹配" class="headerlink" title="联合索引、最左前缀匹配"></a><strong>联合索引、最左前缀匹配</strong></h4><p>Q：不知道的话没关系，想问一下，你们在创建索引的时候都会考虑哪些因素呢？</p>
<p>A：我们一般对于查询概率比较高，经常作为where条件的字段设置索引</p>
<p>Q： 那你们有用过联合索引吗？ </p>
<p>A：用过呀，我们有对一些表中创建过联合索引</p>
<p>Q：那你们在创建联合索引的时候，需要做联合索引多个字段之间顺序你们是如何选择的呢？ </p>
<p>A：我们把识别度最高的字段放到最前面</p>
<p>Q：为什么这么做呢？</p>
<p>A：（这个问题有点把我问蒙了，稍微有些慌乱）这样的话可能命中率会高一点吧。。。</p>
<p>Q： 那你知道最左前缀匹配吗？</p>
<p>A：（我突然想起来原来面试官是想问这个，怪自己刚刚为什么就没想到这个呢。）哦哦哦。您刚刚问的是这个意思啊，在创建多列索引时，我们根据业务需求，where子句中使用最频繁的一列放在最左边，因为MySQL索引查询会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。所以当我们创建一个联合索引的时候，如(key1,key2,key3)，相当于创建了（key1）、(key1,key2)和(key1,key2,key3)三个索引，这就是最左匹配原则</p>
<p>虽然我一开始有点懵，没有联想到最左前缀匹配，但是面试官还是引导了我。很友善。</p>
<h4 id="索引下推、查询优化"><a href="#索引下推、查询优化" class="headerlink" title="索引下推、查询优化"></a><strong>索引下推、查询优化</strong></h4><p>Q：你们线上用的MySQL是哪个版本啊呢？ </p>
<p>A：我们MySQL是5.7 </p>
<p>Q：那你知道在MySQL 5.6中，对索引做了哪些优化吗？ </p>
<p>A：不好意思，这个我没有去了解过。（事后我查了一下，有一个比较重要的 ：Index Condition Pushdown Optimization）</p>
<p>Index Condition Pushdown（索引下推）</p>
<p>MySQL 5.6引入了索引下推优化，默认开启，使用SET optimizer_switch = ‘index_condition_pushdown=off’;可以将其关闭。官方文档中给的例子和解释如下：</p>
<p>people表中（zipcode，lastname，firstname）构成一个索引</p>
<p>SELECT * FROM people WHERE zipcode=’95054’ AND lastname LIKE ‘%etrunia%’ AND address LIKE ‘%Main Street%’;</p>
<p>如果没有使用索引下推技术，则MySQL会通过zipcode=’95054’从存储引擎中查询对应的数据，返回到MySQL服务端，然后MySQL服务端基于lastname LIKE ‘%etrunia%’和address LIKE ‘%Main Street%’来判断数据是否符合条件。</p>
<p>如果使用了索引下推技术，则MYSQL首先会返回符合zipcode=’95054’的索引，然后根据lastname LIKE ‘%etrunia%’和address LIKE ‘%Main Street%’来判断索引是否符合条件。如果符合条件，则根据该索引来定位对应的数据，如果不符合，则直接reject掉。有了索引下推优化，可以在有like条件查询的情况下，减少回表次数。</p>
<p>Q：你们创建的那么多索引，到底有没有生效，或者说你们的SQL语句有没有使用索引查询你们有统计过吗？</p>
<p>A：这个还没有统计过，除非遇到慢SQL的时候我们才会去排查 </p>
<p>Q：那排查的时候，有什么手段可以知道有没有走索引查询呢？</p>
<p>A：可以通过explain查看sql语句的执行计划，通过执行计划来分析索引使用情况</p>
<p>Q：那什么情况下会发生明明创建了索引，但是执行的时候并没有通过索引呢？ </p>
<p>A：（大概记得和优化器有关，但是这个问题并没有回答好）</p>
<h4 id="查询优化器？"><a href="#查询优化器？" class="headerlink" title="查询优化器？"></a>查询优化器？</h4><p>一条SQL语句的查询，可以有不同的执行方案，至于最终选择哪种方案，需要通过优化器进行选择，选择执行成本最低的方案。</p>
<p>在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案。</p>
<p>这个成本最低的方案就是所谓的执行计划。优化过程大致如下：</p>
<p>1、根据搜索条件，找出所有可能使用的索引 </p>
<p>2、计算全表扫描的代价 </p>
<p>3、计算使用不同索引执行查询的代价 </p>
<p>4、对比各种执行方案的代价，找出成本最低的那一个</p>
<p>Q：哦，索引有关的知识我们暂时就问这么多吧。你们线上数据的事务隔离级别是什么呀？ </p>
<p>A：(后面关于事务隔离级别的问题了，就不展开了)<br>Read Uncommitted（读取未提交内容）</p>
<p>在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。</p>
<p>Read Committed（读取提交内容）</p>
<p>这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。</p>
<p>Repeatable Read（可重读）</p>
<p>这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。</p>
<p>Serializable（可串行化）</p>
<p>这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。</p>
<p>出现问题</p>
<p>这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：</p>
<p>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</p>
<p>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</p>
<p>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</p>
<h4 id="查询相关"><a href="#查询相关" class="headerlink" title="查询相关"></a>查询相关</h4>]]></content>
      <categories>
        <category>数据库</category>
        <category>mysql</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-06-28说一说你了解的Mapreduce过程吧</title>
    <url>/2020/06/28/mapreduce/</url>
    <content><![CDATA[<p><img src="/medias/bigdata/maptask.png" alt="maptask"><br><img src="/medias/bigdata/reducetask.png" alt="reducetask"></p>
<h4 id="Map-task"><a href="#Map-task" class="headerlink" title="Map task"></a>Map task</h4><ul>
<li><p>由程序内的InputFormat(默认实现类TextInputFormat)来读取外部数据，它会调用RecordReader(它的成员变量)的read()方法来读取，返回k,v键值对。</p>
</li>
<li><p>读取的k,v键值对传送给map()方法，作为其入参来执行用户定义的map逻辑。</p>
</li>
<li><p>context.write方法被调用时，outputCollector组件会将map()方法的输出结果写入到环形缓冲区内。</p>
</li>
<li><p>环形缓冲区其实就是一个数组，后端不断接受数据的同时，前端数据不断被溢出，长度用完后读取的新数据再从前端开始覆盖。这个缓冲区默认大小100M，可以通过MR.SORT.MB(应该是它)配置。</p>
</li>
<li><p>spiller组件会从环形缓冲区溢出文件，这过程会按照定义的partitioner分区(默认是hashpartition)，并且按照key.compareTo进行排序(底层主要用快排和外部排序)，若有combiner也会执行combiner。spiller的不断工作，会不断溢出许多小文件。这些文件仍在map task所处机器上。</p>
</li>
<li><p>小文件执行merge(合并)，行程分区且区内有序的大文件(归并排序，会再一次调用combiner)。</p>
</li>
<li><p>Reduce会根据自己的分区，去所有map task中，从文件读取对应的数据。</p>
</li>
</ul>
<h4 id="Reduce-task"><a href="#Reduce-task" class="headerlink" title="Reduce task"></a>Reduce task</h4><ul>
<li><p>.reduce task通过网络向map task获取某一分区的数据。</p>
</li>
<li><p>通过GroupingComparator()分辨同一组的数据，把他们发送给reduce(k,iterator)方法</p>
</li>
</ul>
<blockquote>
<p> (这里多个数据合成一组时，只取其中一个key，取得是第一个)。</p>
</blockquote>
<ul>
<li><p>调用context.write()方法，会让OutPurFormat调用RecodeWriter的write()方法将处理结果写入到数据仓库中。写出的只有一个分区的文件数据.</p>
</li>
<li><p>就此mapreduce的工作结束，其中map的context.write()调用后，开始聚合数据写入到reduce的过程叫做Shuffle，是mapreduce的核心所在。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title>说一说你面试中的几种排序</title>
    <url>/2020/06/28/codesort/</url>
    <content><![CDATA[<h4 id="快排："><a href="#快排：" class="headerlink" title="快排："></a>快排：</h4><pre><code>private static void QuickSort(int[] num, int left, int right){
        // 如果left=right，即数组中只剩一个元素时，直接返回
        if (left &gt;= right){
            return;
        }
        // 设置最左边的元素为基准值
        int key = num[left];
        // 数组中比key小的放在左边，比key大的放在右边
      int i = left;
        int j = right;
        while (i &lt; j){
            // j向左移动，直到碰到比key小的数
            while (num[j] &gt;= key &amp;&amp; i&lt;j){
                j--;
            }
            // i向右移动，直到碰到比key大的数
            while (num[i] &lt;= key &amp;&amp; i&lt;j){
                i++;
            }
            // i和j指向的元素交换
            if (i &lt; j){
                int temp = num[i];
                num[i] = num[j];
                num[j] = temp;
            }
        }
        num[left] = num[i];
        num[i] = key;
        QuickSort(num, left, i-1);
        QuickSort(num, i+1, right);
}</code></pre><h4 id="冒泡排序："><a href="#冒泡排序：" class="headerlink" title="冒泡排序："></a>冒泡排序：</h4><pre><code>public static void main(String[] args) {
        //冒泡排序算法
        int[] numbers=new int[]{1,5,8,2,3,9,4};
        int i,j;
        for(i=0;i&lt;numbers.length-1;i++)
        {
            for(j=0;j&lt;numbers.length-1-i;j++)
            {
                if(numbers[j]&gt;numbers[j+1])
                {
                    int temp=numbers[j];
                    numbers[j]=numbers[j+1];
                    numbers[j+1]=temp;
                }
            }
        }
        System.out.println(&quot;从小到大排序后的结果是:&quot;);
        for(i=0;i&lt;numbers.length;i++)
            System.out.print(numbers[i]+&quot; &quot;);
}</code></pre><h4 id="给定两个Node，-找到这两个node的共同的父节点"><a href="#给定两个Node，-找到这两个node的共同的父节点" class="headerlink" title="给定两个Node， 找到这两个node的共同的父节点"></a>给定两个Node， 找到这两个node的共同的父节点</h4><pre><code>function commonParentNode(oNode1, oNode2) {
    var on1_parents = getParents(oNode1);
    var on2_parents = getParents(oNode2);
    var i = on1_parents.length;
    var j = on2_parents.length;
    for(; i &gt;= 0 &amp;&amp; j &gt;= 0 &amp;&amp; on1_parents[i] === on2_parents[j]; i--, j--);
    return on1_parents[i+1];
}

function getParents(oNode) {
    var parents = [];
    var parent = oNode;
    while(parent) {
        parents.push(parent);
        parent = parent.parentElement;
    }
    return parents;
}</code></pre><h4 id="合并两个有序链表"><a href="#合并两个有序链表" class="headerlink" title="合并两个有序链表"></a>合并两个有序链表</h4><pre><code>public static Node mergeTwoList(Node head1, Node head2) {
        //递归结束条件
        if (head1 == null &amp;&amp; head2 == null) {
            return null;
        }
        if (head1 == null) {
            return head2;
        }
        if (head2 == null) {
            return head1;
        }
        //合并后的链表
        Node head = null;
        if (head1.data &gt; head2.data) {
            //把head较小的结点给头结点
            head = head2;
            //继续递归head2
            head.next = mergeTwoList(head1, head2.next);
        } else {
            head = head1;
            head.next = mergeTwoList(head1.next, head2);
        }
        return head;
}</code></pre><h4 id="腾讯的一道面试题：如何快速找到位置长度单链表的中间节点？普通方法，就是先遍历，在从头找到2-length的中间节点。算法复杂度是：O-3-n-2-。而更快的方法就是利用快慢指针的原理。"><a href="#腾讯的一道面试题：如何快速找到位置长度单链表的中间节点？普通方法，就是先遍历，在从头找到2-length的中间节点。算法复杂度是：O-3-n-2-。而更快的方法就是利用快慢指针的原理。" class="headerlink" title="腾讯的一道面试题：如何快速找到位置长度单链表的中间节点？普通方法，就是先遍历，在从头找到2/length的中间节点。算法复杂度是：O(3*n/2)。而更快的方法就是利用快慢指针的原理。"></a>腾讯的一道面试题：如何快速找到位置长度单链表的中间节点？普通方法，就是先遍历，在从头找到2/length的中间节点。算法复杂度是：O(3*n/2)。而更快的方法就是利用快慢指针的原理。</h4><pre><code>int GetMidNode(LinkList *L,int elem){ 
LinkList *search,*mid; 
mid = search = L; //指向头结点 
while (search-&gt;next != NULL){ 
//当存在下个结点的时候 
if (search-&gt;next-&gt;next!=NULL) {
//检查下个的下个节点是否为空 
search = search-&gt;next-&gt;next; 
mid = mid-&gt;next; 
} 
else 
search = search-&gt;next; 
} 
elem = mid-&gt;data;
 return elem; 
}</code></pre>]]></content>
      <categories>
        <category>算法</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title>spark-submit 中几个参数的意义</title>
    <url>/2020/06/29/sparksubmit/</url>
    <content><![CDATA[<h4 id="spark-submit-中几个参数的意义"><a href="#spark-submit-中几个参数的意义" class="headerlink" title="spark-submit 中几个参数的意义"></a>spark-submit 中几个参数的意义</h4><pre><code>1.num-executors  线程数：一般设置在50-100之间，必须设置，不然默认启动的executor非常少，不能充分利用集群资源，运行速度慢
2.executor-memory 线程内存：参考值4g-8g,num-executor乘以executor-memory不能超过队列最大内存，申请的资源最好不要超过最大内存的1/3-1/2
3.executor-cores 线程CPU core数量：core越多，task线程就能快速的分配，参考值2-4，num-executor*executor-cores的1/3-1/2

1.spark-submit spark提交
2.--queue spark 在spark队列
3.--master yarn 在yarn节点提交
4.--deploy-mode client 选择client模型，还是cluster模式；在同一个节点用client,在不同的节点用cluster
5.--executor-memory=4G 线程内存：参考值4g-8g,num-executor乘以executor-memory不能超过队列最大内存，申请的资源最好不要超过最大内存的1/3-1/2
6.--conf spark.dynamicAllocation.enabled=true 是否启动动态资源分配
7.--executor-cores 2 线程CPU core数量：core越多，task线程就能快速的分配，参考值2-4，num-executor*executor-cores的1/3-1/2
8.--conf spark.dynamicAllocation.minExecutors=4 执行器最少数量
9.--conf spark.dynamicAllocation.maxExecutors=10 执行器最大数量
10.--conf spark.dynamicAllocation.initialExecutors=4 若动态分配为true,执行器的初始数量
11.--conf spark.executor.memoryOverhead=2g 堆外内存：处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就去调节这个参数，到至少1G（1024M），甚至说2G、4G）
12.--conf spark.speculation=true 推测执行：在接入kafaka的时候不能使用，需要考虑情景
13.--conf spark.shuffle.service.enabled=true 提升shuffle计算性能</code></pre>]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive有哪些方式保存元数据，各有什么特点？</title>
    <url>/2020/06/29/hivefeature/</url>
    <content><![CDATA[<h4 id="Hive有哪些方式保存元数据，各有什么特点？"><a href="#Hive有哪些方式保存元数据，各有什么特点？" class="headerlink" title="Hive有哪些方式保存元数据，各有什么特点？"></a>Hive有哪些方式保存元数据，各有什么特点？</h4><p>Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器、本地元存储服务器、远程元存储服务器，每种存储方式使用不同的配置参数。</p>
<p>内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。</p>
<p>在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。</p>
<p>在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>谈一下Yarn的工作流程</title>
    <url>/2020/06/29/yarnflow/</url>
    <content><![CDATA[<h4 id="谈一下Yarn的工作流程"><a href="#谈一下Yarn的工作流程" class="headerlink" title="谈一下Yarn的工作流程"></a>谈一下Yarn的工作流程</h4><ul>
<li>用户向Yarn中提交一个MR(MapReduce)任务,由ResourceManager中的Applications Manager接收<br>Applications Manager负责资源的分配, 根据任务计算出所需要的资源,如cpu资源和内存资源,将这些资源封装成Container</li>
<li>Applications Manager将任务和Container发送给Resource Scheduler(资源调度器)<br>Resource Scheduler将任务和Container分配给Application Master, 由Application Master进行二次划分, 将任务分解成MapTask和ReduceTask</li>
<li>Application Master将MapTask和ReduceTask分配给NodeManager,三个NodeManager随机接收到MapTask或者ReduceTask , 由NodeManager负责任务的执行<blockquote>
<p>注意,Application Master会对NodeManager的任务完成情况进行监控, 而Applications Manager会对NodeManager的任务资源使用情况进行监控.<br>如果NodeManager上的任务执行成功,会把成功信息发送给Application Master和Applications Manager, 然后Applications Manager会进行资源的回收.<br>如果NodeManager上的任务执行失败,会把失败信息发送给Application Master和Applications Manager, 然后Applications Manager仍然会进行资源的回收. 此时Application Master会向Applications Manager申请资源, 重新将这个任务分配给这个NodeManager , 循环往复, 直到任务执行成功.</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title>hdfs读写过程，以及数据完整性如何保证？</title>
    <url>/2020/06/29/hdfscheck/</url>
    <content><![CDATA[<h4 id="hdfs读写过程，以及数据完整性如何保证？"><a href="#hdfs读写过程，以及数据完整性如何保证？" class="headerlink" title="hdfs读写过程，以及数据完整性如何保证？"></a>hdfs读写过程，以及数据完整性如何保证？</h4><p>数据完整性的保证，是通过检查分块时候计算出的校验和（隐藏文件里）和读取到的文件块中校验和是否匹配。通过校验和。因为每个chunk中都有一个校验位，一个个chunk构成packet，一个个packet最终形成block，故可在block上求校验和。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark应用程序的执行过程</title>
    <url>/2020/06/29/sparkflow/</url>
    <content><![CDATA[<h4 id="Spark应用程序的执行过程"><a href="#Spark应用程序的执行过程" class="headerlink" title="Spark应用程序的执行过程"></a>Spark应用程序的执行过程</h4><ul>
<li>构建Spark Application的运行环境（启动SparkContext）</li>
<li>SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；</li>
<li>资源管理器分配Executor资源，Executor运行情况将随着心跳发送到资源管理器上；</li>
<li>SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler</li>
<li>Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，SparkContext将应用程序代码发放给Executor。</li>
<li>Task在Executor上运行，运行完毕释放所有资源。</li>
</ul>
<h4 id="spark工作机制"><a href="#spark工作机制" class="headerlink" title="spark工作机制"></a>spark工作机制</h4><ul>
<li>用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。</li>
<li>执行add算子，形成dag图输入dagscheduler</li>
<li>按照add之间的依赖关系划分stage输入task scheduler</li>
<li>ask scheduler会将stage划分为taskset分发到各个节点的executor中执行</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Sspark有哪些组件,以及他们的功能是什么？</title>
    <url>/2020/06/29/sparkinfo/</url>
    <content><![CDATA[<h4 id="spark有哪些组件-以及他们的功能是什么？"><a href="#spark有哪些组件-以及他们的功能是什么？" class="headerlink" title="spark有哪些组件,以及他们的功能是什么？"></a>spark有哪些组件,以及他们的功能是什么？</h4><ul>
<li>master：管理集群和节点，不参与计算。</li>
<li>worker：计算节点，进程本身不参与计算，和master汇报。<ul>
<li>管理当前节点内存，CPU的使用状况，接收master分配过来的资源指令，通过ExecutorRunner启动程序分配任务</li>
<li>worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务</li>
<li>worker不会运行代码，具体运行的是Executor是可以运行具体appliaction写的业务逻辑代码</li>
</ul>
</li>
<li>Driver：运行程序的main方法，创建spark context对象。<ul>
<li>一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext的实例，是程序的人口点；</li>
<li>功能： <ul>
<li>向集群申请资源</li>
<li>负责了作业的调度和解析</li>
<li>生成Stage并调度Task到Executor上（包括DAGScheduler，TaskScheduler）</li>
</ul>
</li>
</ul>
</li>
<li>spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。</li>
<li>client：用户提交程序的入口。</li>
</ul>
<h4 id="spark-一些基础问题"><a href="#spark-一些基础问题" class="headerlink" title="spark 一些基础问题"></a>spark 一些基础问题</h4><h5 id="task有几种类型？2种"><a href="#task有几种类型？2种" class="headerlink" title="task有几种类型？2种"></a>task有几种类型？2种</h5><ul>
<li>resultTask类型，最后一个task</li>
<li>shuffleMapTask类型，除了最后一个task都是</li>
</ul>
<h5 id="什么是shuffle，以及为什么需要shuffle？"><a href="#什么是shuffle，以及为什么需要shuffle？" class="headerlink" title="什么是shuffle，以及为什么需要shuffle？"></a>什么是shuffle，以及为什么需要shuffle？</h5><ul>
<li>shuffle中文翻译为洗牌，需要shuffle的原因是：某种具有共同特征的数据汇聚到一个计算节点上进行计算</li>
</ul>
<h5 id="Spark-master-HA-主从切换过程不会影响集群已有的作业运行，为什么？"><a href="#Spark-master-HA-主从切换过程不会影响集群已有的作业运行，为什么？" class="headerlink" title="Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？"></a>Spark master HA 主从切换过程不会影响集群已有的作业运行，为什么？</h5><ul>
<li>因为程序在运行之前，已经申请过资源了，driver和Executors通讯，不需要和master进行通讯的。</li>
</ul>
<h5 id="Spark中数据的位置是被谁管理的？"><a href="#Spark中数据的位置是被谁管理的？" class="headerlink" title="Spark中数据的位置是被谁管理的？"></a>Spark中数据的位置是被谁管理的？</h5><ul>
<li>每个数据分片都对应具体物理位置，数据的位置是被blockManager管理</li>
</ul>
<h5 id="为什么要进行序列化"><a href="#为什么要进行序列化" class="headerlink" title="为什么要进行序列化"></a>为什么要进行序列化</h5><ul>
<li>减少存储空间，高效存储和传输数据，缺点：使用时需要反序列化，非常消耗CPU</li>
</ul>
<h5 id="Spark如何处理不能被序列化的对象？"><a href="#Spark如何处理不能被序列化的对象？" class="headerlink" title="Spark如何处理不能被序列化的对象？"></a>Spark如何处理不能被序列化的对象？</h5><ul>
<li>封装成object</li>
</ul>
<h5 id="Spark提交你的jar包时所用的命令是什么？"><a href="#Spark提交你的jar包时所用的命令是什么？" class="headerlink" title="Spark提交你的jar包时所用的命令是什么？"></a>Spark提交你的jar包时所用的命令是什么？</h5><ul>
<li>spark-submit</li>
</ul>
<h5 id="Spark并行度怎么设置比较合适"><a href="#Spark并行度怎么设置比较合适" class="headerlink" title="Spark并行度怎么设置比较合适"></a>Spark并行度怎么设置比较合适</h5><ul>
<li>spark并行度，每个core承载2~4个partition（并行度）</li>
<li>并行读和数据规模无关，只和内存和cpu有关</li>
</ul>
<h5 id="Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"><a href="#Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？" class="headerlink" title="Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？"></a>Spark程序执行，有时候默认为什么会产生很多task，怎么修改默认task执行个数？</h5><ul>
<li>有很多小文件的时候，有多少个输入block就会有多少个task启动</li>
<li>spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>简单说一下hadoop和spark的shuffle相同和差异？</title>
    <url>/2020/06/29/mrSparkshuffle/</url>
    <content><![CDATA[<h4 id="简单说一下hadoop和spark的shuffle相同和差异？"><a href="#简单说一下hadoop和spark的shuffle相同和差异？" class="headerlink" title="简单说一下hadoop和spark的shuffle相同和差异？"></a>简单说一下hadoop和spark的shuffle相同和差异？</h4><ul>
<li>high-level 角度： <ul>
<li>两者并没有大的差别 都是将 mapper（Spark: ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）<br>Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce()。</li>
</ul>
</li>
<li>low-level 角度： <ul>
<li>Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。</li>
<li>好处：combine/reduce() 可以处理大规模的数据 <ul>
<li>因为其输入数据可以通过外排得到</li>
<li>mapper 对每段数据先做排序</li>
<li>reducer 的 shuffle 对排好序的每段数据做归并</li>
</ul>
</li>
<li>Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不提前排序</li>
<li>如果用户需要经过排序的数据：sortByKey()</li>
</ul>
</li>
<li>实现角度： <ul>
<li>Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spilt, merge, shuffle, sort, reduce()</li>
<li>Spark 没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，spill, merge, aggregate 等操作需要蕴含在 transformation() 中</li>
</ul>
</li>
</ul>
<h4 id="简单说一下hadoop和spark的shuffle过程"><a href="#简单说一下hadoop和spark的shuffle过程" class="headerlink" title="简单说一下hadoop和spark的shuffle过程"></a>简单说一下hadoop和spark的shuffle过程</h4><ul>
<li>hadoop：map端保存分片数据，通过网络收集到reduce端</li>
<li>spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor，减少shuffle可以提高性能</li>
</ul>
<h4 id="partition和block的关联"><a href="#partition和block的关联" class="headerlink" title="partition和block的关联"></a>partition和block的关联</h4><ul>
<li>hdfs中的block是分布式存储的最小单元，等分，可设置冗余，这样设计有一部分磁盘空间的浪费，但是整齐的block大小，便于快速找到、读取对应的内容</li>
<li>Spark中的partition是RDD的最小单元，RDD是由分布在各个节点上的partition组成的。</li>
<li>partition是指的spark在计算过程中，生成的数据在计算空间内最小单元</li>
<li>同一份数据（RDD）的partion大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定</li>
<li>block位于存储空间；partion位于计 算空间，block的大小是固定的、partion大小是不固定的，是从2个不同的角度去看数据。</li>
</ul>
<h4 id="Spark为什么比mapreduce快？"><a href="#Spark为什么比mapreduce快？" class="headerlink" title="Spark为什么比mapreduce快？"></a>Spark为什么比mapreduce快？</h4><ul>
<li>基于内存计算，减少低效的磁盘交互</li>
<li>高效的调度算法，基于DAG</li>
<li>容错机制Linage</li>
</ul>
<h4 id="Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？"><a href="#Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？" class="headerlink" title="Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？"></a>Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？</h4><ul>
<li>相当于spark中的map算子和reduceByKey算子，区别：MR会自动进行排序的，spark要看具体partitioner</li>
</ul>
<h4 id="Mapreduce和Spark的相同和区别"><a href="#Mapreduce和Spark的相同和区别" class="headerlink" title="Mapreduce和Spark的相同和区别"></a>Mapreduce和Spark的相同和区别</h4><ul>
<li>两者都是用mr模型来进行并行计算</li>
<li>hadoop的一个作业：job <ul>
<li>job分为map task和reduce task，每个task都是在自己的进程中运行的</li>
<li>当task结束时，进程也会结束</li>
</ul>
</li>
<li>spark用户提交的任务：application <ul>
<li>一个application对应一个sparkcontext，app中存在多个job</li>
<li>每触发一次action操作就会产生一个job</li>
<li>这些job可以并行或串行执行</li>
<li>每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的</li>
<li>每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行</li>
<li>executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。</li>
</ul>
</li>
<li>hadoop的job只有map和reduce操作，表达能力比较欠缺 <ul>
<li>在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。</li>
</ul>
</li>
<li>spark的迭代计算都是在内存中进行的 <ul>
<li>API中提供了大量的RDD操作如join，groupby等</li>
<li>通过DAG图可以实现良好的容错</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
        <tag>shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD的一些面试问题</title>
    <url>/2020/06/29/rddinterview/</url>
    <content><![CDATA[<h4 id="RDD机制"><a href="#RDD机制" class="headerlink" title="RDD机制"></a>RDD机制</h4><ul>
<li>分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币</li>
<li>所有算子都是基于rdd来执行的</li>
<li>rdd执行过程中会形成dag图，然后形成lineage保证容错性等</li>
<li>从物理的角度来看rdd存储的是block和node之间的映射</li>
</ul>
<h4 id="RDD的弹性表现在哪几点？"><a href="#RDD的弹性表现在哪几点？" class="headerlink" title="RDD的弹性表现在哪几点？"></a>RDD的弹性表现在哪几点？</h4><ul>
<li>自动的进行内存和磁盘的存储切换；</li>
<li>基于Lingage的高效容错；</li>
<li>task如果失败会自动进行特定次数的重试；</li>
<li>stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；</li>
<li>checkpoint和persist，数据计算之后持久化缓存</li>
<li>数据调度弹性，DAG TASK调度和资源无关</li>
<li>数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par</li>
</ul>
<h4 id="RDD有哪些缺陷？"><a href="#RDD有哪些缺陷？" class="headerlink" title="RDD有哪些缺陷？"></a>RDD有哪些缺陷？</h4><ul>
<li>不支持细粒度的写和更新操作（如网络爬虫） <ul>
<li>spark写数据是粗粒度的，所谓粗粒度，就是批量写入数据 （批量写）</li>
<li>但是读数据是细粒度的也就是说可以一条条的读 （一条条读）</li>
</ul>
</li>
<li>不支持增量迭代计算，Flink支持</li>
</ul>
<h4 id="什么是RDD宽依赖和窄依赖？"><a href="#什么是RDD宽依赖和窄依赖？" class="headerlink" title="什么是RDD宽依赖和窄依赖？"></a>什么是RDD宽依赖和窄依赖？</h4><ul>
<li>RDD和它依赖的parent RDD(s)的关系有两种不同的类型 <ul>
<li>窄依赖：每一个parent RDD的Partition最多被子RDD的一个Partition使用 （一父一子）</li>
<li>宽依赖：多个子RDD的Partition会依赖同一个parent RDD的Partition （一父多子）</li>
</ul>
</li>
</ul>
<h4 id="cache和pesist的区别"><a href="#cache和pesist的区别" class="headerlink" title="cache和pesist的区别"></a>cache和pesist的区别</h4><ul>
<li>cache和persist都是用于缓存RDD，避免重复计算</li>
<li>.cache() == .persist(MEMORY_ONLY)</li>
</ul>
<h4 id="cache后面能不能接其他算子-它是不是action操作？"><a href="#cache后面能不能接其他算子-它是不是action操作？" class="headerlink" title="cache后面能不能接其他算子,它是不是action操作？"></a>cache后面能不能接其他算子,它是不是action操作？</h4><ul>
<li>可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache</li>
<li>cache不是action操作</li>
</ul>
<h4 id="什么场景下要进行persist操作？"><a href="#什么场景下要进行persist操作？" class="headerlink" title="什么场景下要进行persist操作？"></a>什么场景下要进行persist操作？</h4><p>以下场景会使用persist</p>
<ul>
<li>某个步骤计算非常耗时或计算链条非常长，需要进行persist持久化</li>
<li>shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</li>
<li>shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</li>
</ul>
<h4 id="rdd有几种操作类型？三种！！"><a href="#rdd有几种操作类型？三种！！" class="headerlink" title="rdd有几种操作类型？三种！！"></a>rdd有几种操作类型？三种！！</h4><ul>
<li>transformation，rdd由一种转为另一种rdd</li>
<li>action</li>
<li>cronroller，控制算子(cache/persist) 对性能和效率的有很好的支持</li>
</ul>
<h4 id="reduceByKey是不是action？"><a href="#reduceByKey是不是action？" class="headerlink" title="reduceByKey是不是action？"></a>reduceByKey是不是action？</h4><ul>
<li>不是，很多人都会以为是action，reduce rdd是action</li>
</ul>
<h4 id="collect功能是什么，其底层是怎么实现的？"><a href="#collect功能是什么，其底层是怎么实现的？" class="headerlink" title="collect功能是什么，其底层是怎么实现的？"></a>collect功能是什么，其底层是怎么实现的？</h4><ul>
<li>driver通过collect把集群中各个节点的内容收集过来汇总成结果</li>
<li>collect返回结果是Array类型的，合并后Array中只有一个元素，是tuple类型（KV类型的）的。</li>
</ul>
<h4 id="map与flatMap的区别"><a href="#map与flatMap的区别" class="headerlink" title="map与flatMap的区别"></a>map与flatMap的区别</h4><ul>
<li>map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象</li>
<li>flatMap：对RDD每个元素转换，然后再扁平化，将所有的对象合并为一个对象，会抛弃值为null的值</li>
</ul>
<h4 id="列举你常用的action？"><a href="#列举你常用的action？" class="headerlink" title="列举你常用的action？"></a>列举你常用的action？</h4><p><code>collect，reduce,take,count,saveAsTextFile等</code></p>
<h4 id="union操作是产生宽依赖还是窄依赖？"><a href="#union操作是产生宽依赖还是窄依赖？" class="headerlink" title="union操作是产生宽依赖还是窄依赖？"></a>union操作是产生宽依赖还是窄依赖？</h4><ul>
<li>窄依赖</li>
</ul>
<h4 id="Spark累加器有哪些特点？"><a href="#Spark累加器有哪些特点？" class="headerlink" title="Spark累加器有哪些特点？"></a>Spark累加器有哪些特点？</h4><ul>
<li>全局的，只增不减，记录全局集群的唯一状态</li>
<li>在exe中修改它，在driver读取</li>
<li>executor级别共享的，广播变量是task级别的共享</li>
<li>两个application不可以共享累加器，但是同一个app不同的job可以共享</li>
</ul>
<h4 id="spark-hashParitioner的弊端"><a href="#spark-hashParitioner的弊端" class="headerlink" title="spark hashParitioner的弊端"></a>spark hashParitioner的弊端</h4><ul>
<li>分区原理：对于给定的key，计算其hashCode</li>
<li>弊端是数据不均匀，容易导致数据倾斜</li>
</ul>
<h4 id="RangePartitioner分区的原理"><a href="#RangePartitioner分区的原理" class="headerlink" title="RangePartitioner分区的原理"></a>RangePartitioner分区的原理</h4><ul>
<li>尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，也就是说一个分区中的元素肯定都是比另一个分区内的元素小或者大</li>
<li>分区内的元素是不能保证顺序的</li>
<li>简单的说就是将一定范围内的数映射到某一个分区内</li>
</ul>
<h4 id="Spark中的HashShufle的有哪些不足？"><a href="#Spark中的HashShufle的有哪些不足？" class="headerlink" title="Spark中的HashShufle的有哪些不足？"></a>Spark中的HashShufle的有哪些不足？</h4><ul>
<li>shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；</li>
<li>容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息</li>
<li>容易出现数据倾斜，导致OOM</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>rdd</tag>
      </tags>
  </entry>
  <entry>
    <title>spark基础之shuffle机制和原理分析</title>
    <url>/2020/06/29/sparkshffule/</url>
    <content><![CDATA[<h4 id="什么是spark-shuffle"><a href="#什么是spark-shuffle" class="headerlink" title="什么是spark shuffle"></a>什么是spark shuffle</h4><p>a). 我们举个例子reduceByKey会产生shuffle对吧，此算子会把上一个RDD每一个相同key的value聚合在一起形成一个新的value，生成一个新的RDD，类型还是&lt;key,value&gt;形式，这样每一个key对应的就是一个聚合起来的value。<br>b). 每一个key对应的value不一定在同一个partition上，也不可能在同一个节点上，因为RDD是弹性分布式数据集，所以他的partition分布在各个节点上的。<br>c). Shuffle write 中每个map task 必须保证将自己处理的分区中的相同key的数据写到同一个分区文件中，可能会写入多个不同的分区文件。<br>d). Shullfe read 中 reduce task 会从上一个stage的所有 map task 所在的机器上寻找属于自己的那些分区文件，这样就可以保证相同key 的数据可以汇聚到同一个节点上去处理聚合。<br>e). reduce task 是根据key的hash值取模分区数，知道那些key对应的数据属于自己分区。<br>Spark的shuffle其实就是对MR任务shuffle进行了更细粒度的优化，我们直接源码分析</p>
<h4 id="sspark-shuffle有几种"><a href="#sspark-shuffle有几种" class="headerlink" title="sspark shuffle有几种"></a>sspark shuffle有几种</h4><p>a). SortShuffleManager 普通运行机制，此方式会在map端对数据进行排序后落地磁盘。<br>b). SortShuffleManager bypass运行机制，次方式不会对数据进行排序<br>c).如果不需要进行排序操作，可以将spark.shuffle.sort.bypassMergeThreshold参数调大一些，此参数默认为200，reduce task任务个数小于此参数时，默认采用的是bypass机制，对的确不需要排序的作业，我们调大此参数可以减少排序的开销</p>
<h4 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h4><p>首先我们导入spark源码包，找到里面的SortShuffleManager类，打开Structur<br><img src="/medias/bigdata/sparkshuffle1.png" alt="sparkshuffle1"></p>
<p>里面的<br>registerShuffle 注册shuffle，<br>getReader 写数据<br>getWriter 读数据</p>
<p>1、我们首先看registerShuffle</p>
<p><img src="/medias/bigdata/sparkshuffle2.png" alt="sparkshuffle2"><br>图片中 if 判断的第一次判断是否使用bypass机制，我们点进shouldBypassMergeSort会发现</p>
<p><img src="/medias/bigdata/sparkshuffle3.png" alt="sparkshuffle3"><br>第一个判断，如果有map端的combine，返回false，就不会采用bypass机制<br>第二个判断，如果partition的个数小于spark.shuffle.sort.bypassMergeThreshold的参数，才会进行bypass的方式</p>
<p><img src="/medias/bigdata/sparkshuffle4.png" alt="sparkshuffle4"><br>返回当map task 端没有combine 并partition的个数小于200个，就会采用bypass机制</p>
<p><img src="/medias/bigdata/sparkshuffle6.png" alt="sparkshuffle5"><br>接下来判断是否采用序列化的方式进行shuffle，次方式会存的更多，更高效<br><img src="/medias/bigdata/sparkshuffle5.png" alt="sparkshuffle6"><br>点进canUseSerializedShuffle我们可以发现又是一堆判断，<br>第一个判断：如果不支持序列化的文件，则返回false，不能采用序列化的形式写入buffer缓存区<br>第二个判断：如果没有定义分区器返回false，也不能采用序列化的形式写入buffer缓存区<br>第三个判断：如果分区数大于最大shuffle分区输出个数（谷歌翻译出来是这个，我也不知道什么意思），也不能采用序列化的形式写入buffer缓存区<br>以上条件都满足了，就会采用序列化的方式写入缓存区<br><img src="/medias/bigdata/sparkshuffle7.png" alt="sparkshuffle7"><br>最后会采用BaseShuffleHandle的方式写入缓存区</p>
<p>2、我们看getWriter</p>
<p><img src="/medias/bigdata/sparkshuffle8.png" alt="sparkshuffle8"></p>
<p>进入write方法，我们会发现，又是三个判断，<br>第一个：如果是序列化的shuffle方式，会采用UnsafeShuffleWriter对象进行write<br>第二个：如果是bypass机制，会采用BypassMergeSortShuffleWriter对象进行writer<br>第三个：如果是其他的，即采用了默认的SortShuffleWriter对象进行write，此方法和MR的shuffle方式一样。<br>总结：MR的shuffle是spark的shuffle的子集，spark shuffle做了更细粒度的划分</p>
<p>三、reader<br><img src="/medias/bigdata/sparkshuffle9.png" alt="sparkshuffle9"></p>
<p>采用BlockStoreShuffleReader的方式读取数据</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分层</title>
    <url>/2020/06/29/dataware/</url>
    <content><![CDATA[<h3 id="数据分层"><a href="#数据分层" class="headerlink" title="数据分层"></a>数据分层</h3><h4 id="ODS层-operational-data-Store"><a href="#ODS层-operational-data-Store" class="headerlink" title="ODS层(operational data Store)"></a>ODS层(operational data Store)</h4><ul>
<li>概念：操作数据层，源业务系统数据的快照，每个表对应着原业务系统的一个表或者一个日志文件，物理模型与业务系统模型一致。</li>
<li>生成方式：从原始日志、关系型数据库或者接口中抽取加载生成。按天分区，关系型数据按照增量字段时间戳增量抽取，日志型数据或者接口数据按照时间戳字段抽取。</li>
<li>删除方式：保存最近一段时间数据。 </li>
</ul>
<h4 id="DWD层-data-warehouse-detail"><a href="#DWD层-data-warehouse-detail" class="headerlink" title="DWD层( data warehouse detail )"></a>DWD层( data warehouse detail )</h4><ul>
<li>概念：数据明细层，最细粒度的事实数据，用来保存源业务系统数据的快照。主要对ods<br>日志脏数据进行清洗，解决一些数据质量问题和数据的完整度问题。</li>
<li>生成方式：从ODS层加工处理而成，对脏数据进行清洗过滤。</li>
<li>删除方式：永久保存 </li>
</ul>
<h4 id="DWS层-data-warehouse-summary"><a href="#DWS层-data-warehouse-summary" class="headerlink" title="DWS层(data warehouse summary)"></a>DWS层(data warehouse summary)</h4><ul>
<li>概念：数据汇总层 ，根据业务主题设计的最细粒度的汇总层。该层对数据的指标和维度进行标准化,面向主题建模。</li>
<li>生产方式：从dwd、dim层按照业务需求加工细粒度的模型宽表。</li>
<li>删除方式：永久保存 </li>
</ul>
<h4 id="ADS层-application-data-serveice"><a href="#ADS层-application-data-serveice" class="headerlink" title="ADS层(application data serveice)"></a>ADS层(application data serveice)</h4><ul>
<li>概念：数据应用层，完全从需求的角度设计，用于提供后续的报表展示、业务查询、OLAP分析、数据分发等。</li>
<li>生产方式：从dws、dim、dwd层加工的模型宽表。</li>
<li>删除方式：永久保存。 </li>
</ul>
<h4 id="DIM层-data-dimension"><a href="#DIM层-data-dimension" class="headerlink" title="DIM层(data dimension)"></a>DIM层(data dimension)</h4><ul>
<li>概念：数据维表，用于存放字典数据，每个表都包含独立于其他维度表的事实特性。类似地域、渠道等维度表。</li>
<li>生产方式：从dws、dim、dwd层加工的模型宽表。</li>
<li>删除方式：永久保存。 </li>
</ul>
<h4 id="TMP层-data-warehouse-temp"><a href="#TMP层-data-warehouse-temp" class="headerlink" title="TMP层(data warehouse temp)"></a>TMP层(data warehouse temp)</h4><ul>
<li>概念：数据临时层，用于存放临时数据或者验证结果数据。</li>
<li>删除方式：随时删除。</li>
</ul>
<h3 id="模型设计规范"><a href="#模型设计规范" class="headerlink" title="模型设计规范"></a>模型设计规范</h3><h4 id="需求分析-结合具体业务场景，确定需求实现的可行性"><a href="#需求分析-结合具体业务场景，确定需求实现的可行性" class="headerlink" title="需求分析(结合具体业务场景，确定需求实现的可行性)"></a>需求分析(结合具体业务场景，确定需求实现的可行性)</h4><ul>
<li>数据探查阶段：探查源数据数据形态，具体包括以下几个方面：</li>
<li>源表主键是否重复</li>
<li>源表字段空值/异常值的统计，枚举值的范围</li>
<li>源表之间的关联关系</li>
<li>源表名和源表字段含义是否清晰</li>
<li>源表字段的数据格式</li>
</ul>
<h4 id="模型设计-设计产出目标表，具体包括以下几个方面"><a href="#模型设计-设计产出目标表，具体包括以下几个方面" class="headerlink" title="模型设计(设计产出目标表，具体包括以下几个方面)"></a>模型设计(设计产出目标表，具体包括以下几个方面)</h4><ul>
<li>表名和表字段名的命名规范</li>
<li>确定表的粒度、事实和维度</li>
<li>确定业务指标的计算口径</li>
<li>敏感字段的处理</li>
</ul>
<h4 id="模型开发"><a href="#模型开发" class="headerlink" title="模型开发"></a>模型开发</h4><ul>
<li>功能描述</li>
<li>创建人员</li>
<li>创建日期</li>
<li>修改人员</li>
<li>修改日期</li>
<li>业务处理逻辑</li>
</ul>
<h4 id="模型数据验证"><a href="#模型数据验证" class="headerlink" title="模型数据验证"></a>模型数据验证</h4><ul>
<li>对模型各层次加工的数据准确性进行验证，结合线上数据进行验收：<ul>
<li>是否符合模型设计规范：表命名、字段命名、建表规范等</li>
<li>业务主体划分是否合理</li>
<li>是否满足业务应用需求</li>
<li>模型扩展性是否满足需求</li>
</ul>
</li>
</ul>
<h3 id="数据质量监控"><a href="#数据质量监控" class="headerlink" title="数据质量监控"></a>数据质量监控</h3><p><img src="/medias/bigdata/dataquality.png" alt="数据质量监控"></p>
<h3 id="监控指标点"><a href="#监控指标点" class="headerlink" title="监控指标点"></a>监控指标点</h3><ul>
<li>1.接入数据条数波动(近7天均值比较)</li>
<li>2.源系统表结构变更(表名、字段名、字段类型)</li>
<li>3.源系统表计算延迟，导致后续数据接入延迟</li>
<li>4.线上维表新增数值， 仓库未及时更新</li>
<li>5.对接入RDBMS表的主键、外键检查：是否重复</li>
<li>6.重要字段长度检查</li>
<li>7.空值检查</li>
<li>8.重要字段枚举分布检查：离散的，多数是维度字段，可以包含空值检查</li>
<li>9.值域检查：连续的，一般是事实字段，计算检查MAX MIN SUM AVG</li>
<li>10.日期合法性检查：是否有跨天日志，是否有不正常时间日志</li>
</ul>
<h3 id="数据治理工作需要使用到哪些技术和工具？"><a href="#数据治理工作需要使用到哪些技术和工具？" class="headerlink" title="数据治理工作需要使用到哪些技术和工具？"></a>数据治理工作需要使用到哪些技术和工具？</h3><ul>
<li>元数据管理：包括元数据采集、血缘分析、影响分析等功能</li>
<li>数据标准管理：包括标准定义、标准查询、标准发布等功能</li>
<li>数据质量管理：包括质量规则定义、质量检查、质量报告等功能<br><img src="/medias/bigdata/datawarestar.png" alt="datawarestar"><br><img src="/medias/bigdata/datawarekimball.png" alt="datawarekimball"><br><img src="/medias/bigdata/datawareplatform.png" alt="数据平台建设"></li>
</ul>
]]></content>
      <categories>
        <category>数仓</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>shell常见的一些题</title>
    <url>/2020/06/27/shell/</url>
    <content><![CDATA[<h3 id="shell常见的一些题"><a href="#shell常见的一些题" class="headerlink" title="shell常见的一些题"></a>shell常见的一些题</h3><h4 id="shell实现wordcount"><a href="#shell实现wordcount" class="headerlink" title="shell实现wordcount"></a>shell实现wordcount</h4><pre><code>cat words.txt | tr -s &apos; &apos; &apos;\n&apos; | sort | uniq -c |  sort -r | awk &apos;{ print $2, $1 }&apos;</code></pre>]]></content>
      <categories>
        <category>shell</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
</search>
